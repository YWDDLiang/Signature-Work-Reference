<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>References</title>
</head>
<body>
    <h1>References</h1>
    <ol>
        <li>Simon Alexanderson et al. “Listen, Denoise, Action! Audio‐Driven Motion Synthesis with Diffusion Models”. In: 42.4 (July 2023). ıſſN: 0730‐0301. DOı: 10.1145/3592458.</li>
        <li>Junyi Ao et al. SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. 2022. arXiv: 2110.07205 [eess.AS].</li>
        <li>Yatong Bai et al. “ConsistencyTTA: Accelerating Diffusion‐Based Text‐to‐Audio Generation with Consistency Distillation”. In: arXiv preprint arXiv:2309.10740 (2024).</li>
        <li>Matej Božić and Marko Horvat. A Survey of Deep Learning Audio Generation Methods. 2024. arXiv: 2406.00146 [cs.SD].</li>
        <li>Hyung Won Chung et al. “Scaling instruction‐finetuned language models”. In: Journal of Machine Learning Research 25.70 (2024), pp. 1–53.</li>
        <li>Imre Csiszár. “I‐divergence geometry of probability distributions and minimization problems”. In: The annals of probability (1975), pp. 146–158.</li>
        <li>Sander Dieleman, Aäron van den Oord, and Karen Simonyan. The challenge of realistic music generation: modelling raw audio at scale. 2018. arXiv: 1806.10474 [cs.SD].</li>
        <li>DC Dowson and BV666017 Landau. “The Fréchet distance between multivariate normal distributions”. In: Journal of multivariate analysis 12.3 (1982), pp. 450–455.</li>
        <li>Ruibo Fu et al. ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024. 2024. arXiv: 2407.12038 [eess.AS].</li>
        <li>Deepanway Ghosal et al. “Text‐to‐audio generation using instruction‐tuned llm and latent diffusion model”. In: arXiv preprint arXiv:2304.13731 (2023).</li>
        <li>Ian Goodfellow et al. “Generative adversarial networks”. In: Communications of the ACM 63.11 (2020), pp. 139–144.</li>
        <li>Martin Heusel et al. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. 2018. arXiv: 1706.08500 [cs.LG].</li>
        <li>Po‐Yao Huang et al. Masked Autoencoders that Listen. 2023. arXiv: 2207.06405 [cs.SD].</li>
        <li>Rahul Kumar Jaiswal and Rajesh Kumar Dubey. “Concatenative Text‐to‐Speech Synthesis System for Communication Recognition”. In: 2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA). 2021, pp. 867–872. DOı: 10.1109/ICECA52323.2021.9675855.</li>
        <li>Kevin Karplus and Alex Strong. “Digital synthesis of plucked‐string and drum timbres”. In: Computer Music Journal 7.2 (1983), pp. 43–55.</li>
        <li>Kevin Kilgour et al. Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. 2019. arXiv: 1812.08466 [eess.AS].</li>
        <li>Chris Dongjoo Kim et al. “AudioCaps: Generating Captions for Audios in The Wild”. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 119–132. DOı: 10.18653/v1/N19‐1011.</li>
        <li>Jaehyeon Kim, Jungil Kong, and Juhee Son. “Conditional variational autoencoder with adversarial learning for end‐to‐end text‐to‐speech”. In: International Conference on Machine Learning. PMLR. 2021, pp. 5530–5540.</li>
        <li>Diederik P Kingma, Max Welling, et al. “An introduction to variational autoencoders”. In: Foundations and Trends® in Machine Learning 12.4 (2019), pp. 307–392.</li>
        <li>Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. “Hifi‐gan: Generative adversarial networks for efficient and high fidelity speech synthesis”. In: Advances in neural information processing systems 33 (2020), pp. 17022–17033.</li>
        <li>Qiuqiang Kong et al. Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation. 2021. arXiv: 2109.05418 [cs.SD].</li>
        <li>Qiuqiang Kong et al. “Panns: Large‐scale pretrained audio neural networks for audio pattern recognition”. In: IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), pp. 2880–2894.</li>
        <li>Karolina Kuligowska, Paweł Kisielewicz, and Aleksandra Włodarz. “Speech synthesis systems: disadvantages and limitations”. In: International Journal of Research in Engineering and Technology (UAE) 7 (2018), pp. 234–239.</li>
        <li>Yinghao Aaron Li et al. “StyleTTS 2: Towards Human‐Level Text‐to‐Speech through Style Diffusion and Adversarial Training with Large Speech Language Models”. In: Advances in Neural Information Processing Systems. Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc., 2023, pp. 19594–19621.</li>
        <li>Dan Lim, Sunghee Jung, and Eesung Kim. JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech. 2022. arXiv: 2203.16852 [eess.AS].</li>
        <li>Haohe Liu et al. “AudioLDM 2: Learning Holistic Audio Generation With Self‐Supervised Pretraining”. In:IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024), pp. 2871–2883. DOı: 10.1109/TASLP.2024.3399607.</li>
        <li>Haohe Liu et al. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. 2023. arXiv: 2301.12503 [cs.SD].</li>
        <li>Haohe Liu et al. “AudioLDM: Text‐to‐Audio Generation with Latent Diffusion Models”. In: Proceedings of the International Conference on Machine Learning (2023), pp. 21450–21474.</li>
        <li>Huadai Liu et al. AudioLCM: Text-to-Audio Generation with Latent Consistency Models. 2024. arXiv: 2406.00356 [eess.AS].</li>
        <li>Daniel Lobo et al. “Emotionally relevant background music generation for audiobooks”. In: 2021 International Conference on Artificial Intelligence and Machine Vision (AIMV). 2021, pp. 1–6. DOı: 10.1109/AIMV53313.2021.9670959.</li>
        <li>Justin Lovelace et al. “Latent Diffusion for Language Generation”. In: Advances in Neural Information Processing Systems. Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc., 2023, pp. 56998–57025.</li>
        <li>Simian Luo et al. Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. 2023. arXiv: 2310.04378 [cs.CV].</li>
        <li>Navonil Majumder et al. Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization. 2024. arXiv: 2404.09956 [cs.SD].</li>
        <li>Soroush Mehri et al. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model. 2017. arXiv: 1612.07837 [cs.SD].</li>
        <li>Ambuj Mehrish et al. A Review of Deep Learning Techniques for Speech Processing. 2023. arXiv: 2305.00359 [eess.AS].</li>
        <li>Bonan Min et al. “Recent advances in natural language processing via large pre‐trained language models: A survey”. In: ACM Computing Surveys 56.2 (2023), pp. 1–40.</li>
        <li>Anastasia Natsiou and Sean O’Leary. Audio representations for deep learning in sound synthesis: A review. 2022. arXiv: 2201.02490 [cs.SD].</li>
        <li>Anastasia Natsiou and Seán O’Leary. “Audio representations for deep learning in sound synthesis: A review”. In: 2021 IEEE/ACS 18th International Conference on Computer Systems and Applications (AICCSA). IEEE. 2021, pp. 1–8.</li>
        <li>Humza Naveed et al. A Comprehensive Overview of Large Language Models. 2024. arXiv: 2307.06435 [cs.CL].</li>
        <li>Aaron van den Oord et al. WaveNet: A Generative Model for Raw Audio. 2016. arXiv: 1609.03499 [cs.SD].</li>
        <li>William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. 2023. arXiv: 2212.09748 [cs.CV].</li>
        <li>Geoffroy Peeters and Gaël Richard. “Deep learning for audio and music”. In: Multi-faceted Deep Learning: Models and Data (2021), pp. 231–266.</li>
        <li>Wei Ping et al. Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning. 2018. arXiv: 1710.07654 [cs.SD].</li>
        <li>Robin Rombach et al. High-Resolution Image Synthesis with Latent Diffusion Models. 2022. arXiv: 2112.10752 [cs.CV].</li>
        <li>Robin Rombach et al. “High‐resolution image synthesis with latent diffusion models”. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 10684–10695.</li>
        <li>Jonathan Shen et al. Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. 2018. arXiv: 1712.05884 [cs.CL].</li>
        <li>Kai Shen et al. NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. 2023. arXiv: 2304.09116 [eess.AS].</li>
        <li>Yang Song et al. Score-Based Generative Modeling through Stochastic Differential Equations. 2021. arXiv: 2011.13456 [cs.LG].</li>
        <li>Lars St, Svante Wold, et al. “Analysis of variance (ANOVA)”. In: Chemometrics and intelligent laboratory systems 6.4 (1989), pp. 259–272.</li>
        <li>Haobin Tang et al. “QI‐TTS: Questioning Intonation Control for Emotional Speech Synthesis”. In:ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023, pp. 1–5. DOı: 10.1109/ICASSP49357.2023.10095623.</li>
        <li>Andros Tjandra et al. VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for Zerospeech Challenge 2019. 2019. arXiv: 1905.11449 [cs.CL].</li>
        <li>A Vaswani. “Attention is all you need”. In: Advances in Neural Information Processing Systems (2017).</li>
        <li>Prateek Verma and Chris Chafe. A Generative Model for Raw Audio Using Transformer Architectures. 2021. arXiv: 2106.16036 [cs.SD].</li>
        <li>Chengyi Wang et al. Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. 2023. arXiv: 2301.02111 [cs.CL].</li>
        <li>Yusong Wu et al. Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. 2024. arXiv: 2211.06687 [cs.SD].</li>
        <li>Dongchao Yang et al. “Diffsound: Discrete diffusion model for text‐to‐sound generation”. In: IEEE/ACM Transactions on Audio, Speech, and Language Processing 31 (2023), pp. 1720–1733.</li>
        <li>Ling Yang et al. Diffusion Models: A Comprehensive Survey of Methods and Applications. 2024. arXiv: 2209.00796 [cs.LG].</li>
        <li>Yi Yuan et al. Text-Driven Foley Sound Generation With Latent Diffusion Model. 2023. arXiv: 2306.10359 [cs.SD].</li>
        <li>Heiga Zen, Keiichi Tokuda, and Alan W Black. “Statistical parametric speech synthesis”. In: speech communication 51.11 (2009), pp. 1039–1064.</li>
        <li>Wayne Xin Zhao et al. “A survey of large language models”. In: arXiv preprint arXiv:2303.18223 1.2 (2023).</li>
        <li>Yuanjun Zhao, Xianjun Xia, and Roberto Togneri. “Applications of deep learning to audio generation”. In: IEEE Circuits and Systems Magazine 19.4 (2019), pp. 19–38.</li>
    </ol>
</body>
</html>
